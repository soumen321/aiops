{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36ec5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643cdac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhatt\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embd = OpenAIEmbeddings()\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-adv-attack/llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=doc_splits, embedding=embd)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58663887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='web_search'\n"
     ]
    }
   ],
   "source": [
    "# Router\n",
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data Model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    \n",
    "    datasource: Literal[\"vectorstore\",\"web_search\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectostore\",\n",
    "    )\n",
    "\n",
    " # LLM with function calling\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "#prompt\n",
    "system = \"\"\"\n",
    "You are an expert routing system for a vectorstore and a web search.\n",
    "You will be given a user question. You need to decide whether to route the user to a vectorstore or a web search.\n",
    "If the user question is related to a vectorstore, route the user to a vectorstore.\n",
    "If the user question is not related to a vectorstore, route the user to a web search.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = prompt | structured_llm_router\n",
    "\n",
    "print(question_router.invoke({\"question\": \"who won the cricket world cup 2023?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca974c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"what are the types of agent memory\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ea4e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhatt\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_openai\\chat_models\\base.py:1963: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "#Retriver Grader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "#Data Model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "    \n",
    "# llm with function calling\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "#prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \n",
    "  If the document contains keywords related to the user question, grade it as relevant. \n",
    "  Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "        (\"human\", \"Retrieved document: {document}\"),\n",
    "    ]\n",
    ")\n",
    "#chain\n",
    "retrival_gender = prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_text = docs[1].page_content  \n",
    "print(retrival_gender.invoke({\"question\": question, \"document\": doc_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39173da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The use of LLMs for evaluating performance in domains requiring deep expertise may lead to inaccuracies due to the lack of expertise in the model. Boiko et al. (2023) explored LLM-empowered agents for scientific discovery, enabling them to handle complex scientific experiments autonomously. Generative Agents combine LLM with memory, planning, and reflection mechanisms to create believable human behavior simulacra.\n"
     ]
    }
   ],
   "source": [
    "#Generate\n",
    "from langchain_classic import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#chain\n",
    "rag_chain = prompt | llm | StrOutputParser() \n",
    "\n",
    "#run\n",
    "generation = rag_chain.invoke({\"context\": format_docs(docs), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c695af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### HAlluciation Grader\n",
    "\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for halluciations present in generation answer.\"\"\"\n",
    "    \n",
    "    binary_score:str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' and 'no'\"        \n",
    "    )\n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0)\n",
    "\n",
    "structured_llm_generator = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "#prompt\n",
    "system = \"\"\"\n",
    "        You are a grader assessing whetheran LLM generation is grounded in / supported by a set of retrived facts/ \\n\n",
    "        GIve a binary score 'yes' or 'no' , 'yes' means  that the answer is grounded in / supported by the set of facts.\n",
    "\"\"\"\n",
    "\n",
    "halluciation_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),       \n",
    "        (\"human\", \"Retrieved document: {document}\"),\n",
    "        (\"human\", \"LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "halluciation_grader = halluciation_prompt | structured_llm_generator\n",
    "halluciation_grader.invoke({\"document\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c87d6c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score for whether the answer is useful.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is useful, 'yes' or 'no'\"\n",
    "    )\n",
    "    \n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_generator = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "system = \"\"\"\n",
    "        You are a grader assessing whether an LLM answer is useful to resolve a question. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by the set of facts.\n",
    "\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "        (\"human\", \"LLM answer: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_generator\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79b8d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question re-write\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "system = \"\"\"\n",
    "        You are generating questions that is well optimized for vectorstore retrieval. \\n\n",
    "        Look at the input and try to reason about the underlying sematic interaction / logic. \\n\n",
    "        Here is the question:\n",
    "        {question}\n",
    "\"\"\"\n",
    "\n",
    "question_rewrite_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = question_rewrite_prompt | llm | StrOutputParser()\n",
    "rewritten_question = question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b43da73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhatt\\AppData\\Local\\Temp\\ipykernel_35424\\4048507736.py:7: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  web_seatch_tool = TavilySearchResults(k=3)\n"
     ]
    }
   ],
   "source": [
    "# web search\n",
    "\n",
    "### Serach\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_seatch_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e702d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating state graph\n",
    "from typing import TypedDict,List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question:question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146da42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
